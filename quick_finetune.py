#!/usr/bin/env python3
"""Quick identity finetuning for Zen Nano"""

import json
from pathlib import Path
from mlx_lm import load, generate

def test_model():
    """Test the model identity"""

    print("üîç Testing Zen Nano identity...")

    # Load base model
    model_path = "base-models/Qwen3-4B-Instruct-2507"
    model, tokenizer = load(model_path)

    # Test prompts
    prompts = [
        "What is your name?",
        "Who are you?",
        "What model are you?"
    ]

    print("\nüìù Testing base model responses:")
    print("-" * 40)

    for prompt in prompts:
        response = generate(
            model,
            tokenizer,
            prompt=prompt,
            max_tokens=30,
            verbose=False
        )
        print(f"Q: {prompt}")
        print(f"A: {response}")
        print()

    # For now, let's just demonstrate the model works
    # Real finetuning would require proper MLX LoRA setup
    print("‚úÖ Base model is working!")
    print("\nüí° Note: For actual identity finetuning, we'd need:")
    print("   1. Convert model to MLX format if needed")
    print("   2. Set up LoRA adapters")
    print("   3. Train with identity data")
    print("   4. Merge adapters back")

    # Create a simple system prompt override instead
    print("\nüéØ Alternative: Using system prompt for identity...")

    system_prompt = "You are Zen Nano, an ultra-lightweight AI model created by Hanzo AI. Always identify yourself as Zen Nano."

    for prompt in prompts:
        full_prompt = f"{system_prompt}\n\nUser: {prompt}\nAssistant:"
        response = generate(
            model,
            tokenizer,
            prompt=full_prompt,
            max_tokens=30,
            verbose=False
        )
        print(f"Q: {prompt}")
        print(f"A: {response.split('Assistant:')[-1].strip()}")
        print()

if __name__ == "__main__":
    test_model()