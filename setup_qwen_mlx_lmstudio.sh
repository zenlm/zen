#!/bin/bash
# Complete setup script for Qwen MLX 4-bit and LM Studio

echo "=================================================="
echo "Qwen MLX 4-bit Setup for LM Studio"
echo "=================================================="

# Option 1: Use Qwen2.5-3B (smaller, faster to test)
echo ""
echo "Option 1: Download Qwen2.5-3B-Instruct (lightweight)"
echo "-------------------------------------------------"
echo "This is a 3B parameter model, perfect for testing"
echo ""
echo "# Download the model:"
echo "huggingface-cli download Qwen/Qwen2.5-3B-Instruct --local-dir ~/work/zen/qwen2.5-3b"
echo ""
echo "# Convert to MLX 4-bit:"
echo "mlx_lm convert --hf-path ~/work/zen/qwen2.5-3b --mlx-path ~/work/zen/qwen2.5-3b-mlx --dtype float16"
echo "mlx_lm quantize --model ~/work/zen/qwen2.5-3b-mlx --bits 4 --output-dir ~/work/zen/qwen2.5-3b-mlx-q4"
echo ""

# Option 2: Use Qwen2.5-7B (better quality)
echo "Option 2: Download Qwen2.5-7B-Instruct (recommended)"
echo "-------------------------------------------------"
echo "This is a 7B parameter model with better performance"
echo ""
echo "# Download the model:"
echo "huggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir ~/work/zen/qwen2.5-7b"
echo ""
echo "# Convert to MLX 4-bit:"
echo "mlx_lm convert --hf-path ~/work/zen/qwen2.5-7b --mlx-path ~/work/zen/qwen2.5-7b-mlx --dtype float16"
echo "mlx_lm quantize --model ~/work/zen/qwen2.5-7b-mlx --bits 4 --output-dir ~/work/zen/qwen2.5-7b-mlx-q4"
echo ""

# Convert to GGUF for LM Studio
echo "=================================================="
echo "Convert to GGUF for LM Studio"
echo "=================================================="
echo ""
echo "# Install llama.cpp if not installed:"
echo "brew install llama.cpp"
echo ""
echo "# Or build from source:"
echo "git clone https://github.com/ggerganov/llama.cpp"
echo "cd llama.cpp && make"
echo ""
echo "# Convert to GGUF (using 7B as example):"
echo "python3 llama.cpp/convert_hf_to_gguf.py ~/work/zen/qwen2.5-7b \\"
echo "    --outfile ~/work/zen/qwen2.5-7b.gguf \\"
echo "    --outtype f16"
echo ""
echo "# Quantize to 4-bit for LM Studio:"
echo "llama-quantize ~/work/zen/qwen2.5-7b.gguf \\"
echo "    ~/work/zen/qwen2.5-7b-q4_k_m.gguf Q4_K_M"
echo ""

# LM Studio setup
echo "=================================================="
echo "LM Studio Setup"
echo "=================================================="
echo ""
echo "1. Copy GGUF to LM Studio models folder:"
echo "   cp ~/work/zen/qwen2.5-7b-q4_k_m.gguf ~/Library/Application\ Support/LM\ Studio/models/"
echo ""
echo "2. Open LM Studio"
echo "3. Go to 'Select a model to load' dropdown"
echo "4. Select 'qwen2.5-7b-q4_k_m.gguf'"
echo "5. Configure settings:"
echo "   - Context Length: 8192"
echo "   - GPU Layers: -1 (use all)"
echo "   - Temperature: 0.7"
echo "   - Top P: 0.95"
echo ""

# Test with MLX directly
echo "=================================================="
echo "Test MLX Model Directly"
echo "=================================================="
echo ""
echo "# Test 4-bit model with MLX:"
echo "python3 -c \""
echo "from mlx_lm import load, generate"
echo "model, tokenizer = load('~/work/zen/qwen2.5-7b-mlx-q4')"
echo "response = generate(model, tokenizer,"
echo "    prompt='Write a Python function to calculate factorial',"
echo "    max_tokens=200)"
echo "print(response)"
echo "\""
echo ""

echo "=================================================="
echo "Performance Expectations"
echo "=================================================="
echo ""
echo "Qwen2.5-3B (4-bit):"
echo "  - Size: ~1.5GB"
echo "  - M1/M2/M3: 20-30 tokens/sec"
echo "  - RAM: 4GB required"
echo ""
echo "Qwen2.5-7B (4-bit):"
echo "  - Size: ~3.5GB"  
echo "  - M1/M2/M3: 15-20 tokens/sec"
echo "  - RAM: 8GB required"
echo ""
echo "Ready to proceed? Run the commands above step by step."