{
  "model_name": "zen-next",
  "metadata": {
    "model_name": "zen-next",
    "model_type": "next-generation",
    "version": "3.0.0-alpha",
    "has_thinking": true,
    "special_tokens": {
      "<s>": "bos_token",
      "</s>": "eos_token",
      "<pad>": "pad_token",
      "<thinking>": "thinking_start",
      "</thinking>": "thinking_end",
      "<|system|>": "system_start",
      "<|/system|>": "system_end",
      "<|tools|>": "tools_start",
      "<|/tools|>": "tools_end",
      "<|memory|>": "memory_start",
      "<|/memory|>": "memory_end"
    },
    "context_length": 65536,
    "vocabulary_size": 60000,
    "hidden_size": 3072,
    "num_layers": 32,
    "num_heads": 24,
    "author": "Hanzo AI",
    "license": "Apache-2.0",
    "base_model": "Custom architecture",
    "capabilities": [
      "advanced-reasoning",
      "tool-use",
      "long-context",
      "multi-turn-memory",
      "code-generation",
      "multimodal"
    ],
    "training_data": "Hanzo Next-Gen training corpus",
    "quantization_info": null
  },
  "conversion_settings": {
    "context_size": 65536,
    "vocab_type": "bpe",
    "use_f16": true,
    "quantization_options": {
      "mobile": [
        "Q4_K_S",
        "Q4_K_M"
      ],
      "balanced": [
        "Q5_K_M",
        "Q4_K_M"
      ],
      "quality": [
        "Q6_K",
        "Q8_0"
      ],
      "server": [
        "Q8_0",
        "FP16"
      ]
    }
  },
  "gguf_metadata": {
    "general.name": "zen-next",
    "general.architecture": "next_generation",
    "general.author": "Hanzo AI",
    "general.version": "3.0.0-alpha",
    "general.license": "Apache-2.0",
    "general.description": "Zen next-generation model by Hanzo AI",
    "next-generation.context_length": 65536,
    "next-generation.embedding_length": 3072,
    "next-generation.block_count": 32,
    "next-generation.attention.head_count": 24,
    "next-generation.vocabulary_size": 60000,
    "tokenizer.ggml.model": "gpt2",
    "tokenizer.ggml.bos_token_id": 1,
    "tokenizer.ggml.eos_token_id": 2,
    "tokenizer.ggml.padding_token_id": 0,
    "tokenizer.ggml.unknown_token_id": 3,
    "general.has_thinking": "true",
    "tokenizer.special_tokens": "{\"<thinking>\": \"thinking_start\", \"</thinking>\": \"thinking_end\"}"
  }
}