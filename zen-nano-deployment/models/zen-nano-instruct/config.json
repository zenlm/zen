{
  "architectures": [
    "ZenNanoForCausalLM"
  ],
  "model_type": "zen_nano",
  "torch_dtype": "float16",
  "transformers_version": "4.36.0",
  "variant": "instruct",

  "vocab_size": 152064,
  "hidden_size": 2560,
  "intermediate_size": 13824,
  "num_hidden_layers": 36,
  "num_attention_heads": 20,
  "num_key_value_heads": 5,
  "head_dim": 128,
  "max_position_embeddings": 32768,
  "sliding_window": 32768,
  "max_window_layers": 28,

  "hidden_act": "silu",
  "initializer_range": 0.02,
  "rms_norm_eps": 1e-6,
  "use_cache": true,
  "use_sliding_window": false,
  "tie_word_embeddings": true,
  "rope_theta": 1000000.0,
  "rope_scaling": {
    "type": "dynamic",
    "factor": 2.0
  },

  "attention_bias": true,
  "attention_dropout": 0.0,
  "mlp_bias": false,

  "optimization_features": {
    "grouped_query_attention": true,
    "adaptive_attention_patterns": true,
    "mixture_of_depths": true,
    "weight_sharing": true,
    "quantization_aware": true,
    "sparse_attention": false
  },

  "inference_optimization": {
    "use_flash_attention": true,
    "use_fused_ops": true,
    "kv_cache_compression": true,
    "dynamic_batching": true
  },

  "quantization_config": {
    "supports_int8": true,
    "supports_int4": true,
    "calibration_dataset": "c4",
    "quantization_method": "gptq"
  },

  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "pad_token_id": 151643,

  "generation_config": {
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "do_sample": true,
    "max_new_tokens": 2048,
    "repetition_penalty": 1.1
  },

  "model_metadata": {
    "model_name": "zen-nano-instruct",
    "model_version": "1.1.0",
    "release_date": "2024-02",
    "base_model": "qwen3-4b-2507",
    "parameters": 4000000000,
    "license": "apache-2.0",
    "languages": ["en", "zh", "es", "fr", "de", "ja", "ko", "ru"],
    "capabilities": [
      "text-generation",
      "instruction-following",
      "code-generation",
      "question-answering",
      "summarization"
    ]
  },

  "training_info": {
    "dataset_tokens": "2.85T",
    "training_steps": 500000,
    "batch_size": 4194304,
    "learning_rate": 3e-4,
    "warmup_steps": 25000,
    "hardware": "64x A100 80GB",
    "training_time_days": 21
  }
}