{
  "model_name": "zen-omni-thinking",
  "metadata": {
    "model_name": "zen-omni-thinking",
    "model_type": "multimodal-reasoning",
    "version": "2.0.0",
    "has_thinking": true,
    "special_tokens": {
      "<s>": "bos_token",
      "</s>": "eos_token",
      "<pad>": "pad_token",
      "<|thinking|>": "thinking_start",
      "<|/thinking|>": "thinking_end",
      "<|image|>": "image_start",
      "<|/image|>": "image_end",
      "<|reasoning|>": "reasoning_start",
      "<|/reasoning|>": "reasoning_end"
    },
    "context_length": 32768,
    "vocabulary_size": 50000,
    "hidden_size": 1536,
    "num_layers": 24,
    "num_heads": 16,
    "author": "Hanzo AI",
    "license": "Apache-2.0",
    "base_model": "Qwen3-VL",
    "capabilities": [
      "multimodal-reasoning",
      "visual-thinking",
      "complex-analysis"
    ],
    "training_data": "Hanzo multimodal reasoning dataset",
    "quantization_info": null
  },
  "conversion_settings": {
    "context_size": 32768,
    "vocab_type": "bpe",
    "use_f16": true,
    "quantization_options": {
      "mobile": [
        "Q4_K_S",
        "Q4_K_M"
      ],
      "balanced": [
        "Q5_K_M",
        "Q4_K_M"
      ],
      "quality": [
        "Q6_K",
        "Q8_0"
      ],
      "server": [
        "Q8_0",
        "FP16"
      ]
    }
  },
  "gguf_metadata": {
    "general.name": "zen-omni-thinking",
    "general.architecture": "multimodal_reasoning",
    "general.author": "Hanzo AI",
    "general.version": "2.0.0",
    "general.license": "Apache-2.0",
    "general.description": "Zen multimodal-reasoning model by Hanzo AI",
    "multimodal-reasoning.context_length": 32768,
    "multimodal-reasoning.embedding_length": 1536,
    "multimodal-reasoning.block_count": 24,
    "multimodal-reasoning.attention.head_count": 16,
    "multimodal-reasoning.vocabulary_size": 50000,
    "tokenizer.ggml.model": "gpt2",
    "tokenizer.ggml.bos_token_id": 1,
    "tokenizer.ggml.eos_token_id": 2,
    "tokenizer.ggml.padding_token_id": 0,
    "tokenizer.ggml.unknown_token_id": 3,
    "general.has_thinking": "true",
    "tokenizer.special_tokens": "{\"<|thinking|>\": \"thinking_start\", \"<|/thinking|>\": \"thinking_end\"}"
  }
}