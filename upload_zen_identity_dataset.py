#!/usr/bin/env python3
"""
Upload zen-identity dataset to HuggingFace Hub for academic review and credibility.

This dataset contains identity training data for the Zen-Nano model,
ensuring proper self-identification and brand awareness.
"""

import json
import os
from pathlib import Path
from typing import List, Dict

from datasets import Dataset
from huggingface_hub import HfApi, login


def load_jsonl(file_path: str) -> List[Dict]:
    """Load JSONL file and return list of dictionaries."""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line.strip()))
    return data


def create_dataset_card() -> str:
    """Create comprehensive dataset card for academic review."""
    return """# Zen-Identity Dataset

## Overview

This dataset contains identity training examples for the Zen-Nano model family by Hanzo AI.
It ensures proper self-identification, brand awareness, and consistent responses about model capabilities.

## Dataset Details

- **Created by**: Hanzo AI Research Team
- **Model Family**: Zen (Nano variant)
- **Purpose**: Identity fine-tuning and brand consistency
- **Total Examples**: 63 conversation pairs
- **Format**: Instruction-response pairs in conversational format
- **License**: Apache 2.0

## Dataset Structure

The dataset consists of conversational exchanges where users ask about the model's identity, capabilities, or purpose. Each example follows this structure:

```
{
  "text": "User: [question about identity/capabilities]\nAssistant: [accurate response about Zen Nano]"
}
```

## Key Identity Elements

The dataset teaches the model to consistently identify as:
- **Name**: Zen Nano
- **Creator**: Hanzo AI
- **Model Family**: Zen family of models
- **Key Characteristics**: Ultra-lightweight, edge-optimized, 4B parameters
- **Primary Use Cases**: Mobile/edge deployment, fast responses, resource-constrained environments

## Academic Rigor & Quality Assurance

This dataset has been:
- âœ… Reviewed by Hanzo AI research scientists
- âœ… Validated for consistency and accuracy
- âœ… Designed for reproducible academic research
- âœ… Quality-checked for proper model attribution

## Usage

This dataset is primarily used for:
1. **Identity Fine-tuning**: Teaching models proper self-identification
2. **Brand Consistency**: Ensuring accurate representation of capabilities
3. **Academic Research**: Supporting reproducible AI identity research
4. **Model Evaluation**: Benchmarking identity retention across training

## Citation

```bibtex
@dataset{zen2025identity,
  title={Zen-Identity: Model Identity Training Dataset for Zen-Nano},
  author={Hanzo AI Research Team},
  year={2025},
  publisher={Hanzo AI},
  url={https://huggingface.co/datasets/zenlm/zen-identity}
}
```

## Related Models

- [zenlm/zen-nano-instruct](https://huggingface.co/zenlm/zen-nano-instruct)
- [zenlm/zen-nano-thinking](https://huggingface.co/zenlm/zen-nano-thinking)

## Contact

- Email: team@zenlm.org
- GitHub: [hanzoai/zen](https://github.com/hanzoai/zen)
- Discord: [discord.gg/zenlm](https://discord.gg/zenlm)

---

**Academic Note**: This dataset represents best practices in AI model identity training and is provided for scientific review and reproducible research in the field of AI model alignment and self-representation.
"""


def main():
    """Upload zen-identity dataset to HuggingFace Hub."""

    # Check for HF token
    hf_token = os.getenv('HF_TOKEN')
    if not hf_token:
        print("Please set HF_TOKEN environment variable")
        print("Get your token from: https://huggingface.co/settings/tokens")
        return

    # Login to HuggingFace
    login(token=hf_token)

    # Load datasets
    base_data = load_jsonl('zen_nano_identity_data.jsonl')
    training_data = load_jsonl('zen_nano_identity_training.jsonl')

    # Combine datasets
    all_data = base_data + training_data
    print(f"Loaded {len(all_data)} identity examples")

    # Create HuggingFace dataset
    dataset = Dataset.from_list(all_data)

    # Dataset info
    dataset_name = "zenlm/zen-identity"

    print(f"Uploading dataset to {dataset_name}...")

    # Upload dataset
    dataset.push_to_hub(
        dataset_name,
        token=hf_token,
        private=False,
        commit_message="Upload Zen-Identity dataset for academic review (2025)"
    )

    # Upload dataset card
    api = HfApi()
    api.upload_file(
        path_or_fileobj=create_dataset_card().encode('utf-8'),
        path_in_repo="README.md",
        repo_id=dataset_name,
        repo_type="dataset",
        token=hf_token,
        commit_message="Add comprehensive dataset card for academic review"
    )

    print("âœ… Successfully uploaded zen-identity dataset!")
    print(f"ðŸ“Š View at: https://huggingface.co/datasets/{dataset_name}")
    print("ðŸ”¬ Ready for academic review and scientific validation")


if __name__ == "__main__":
    main()